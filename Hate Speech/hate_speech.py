# -*- coding: utf-8 -*-
"""Hate Speech.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z8A4w4HYWd3P2V0B-83RBGuprIqn2Okx
"""

!pip install transformers streamlit

import pandas as pd

url = "https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/data/labeled_data.csv"
df = pd.read_csv(url)
df['binary_label'] = df['class'].apply(lambda x: 0 if x == 0 else 1)
print(df['binary_label'].value_counts())

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import joblib

# Binary labels: 0=hate, 1=clean (offensive+neither)
df['binary_label'] = df['class'].apply(lambda x: 1 if x != 0 else 0)

# Split data
X_train, X_test, y_train, y_test = train_test_split(df['tweet'], df['binary_label'], test_size=0.2, random_state=42)

# Vectorize text
vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# Train model
model = LogisticRegression(class_weight='balanced')
model.fit(X_train_vec, y_train)

# Evaluate
pred = model.predict(X_test_vec)
print("Accuracy:", accuracy_score(y_test, pred))
# Save model and vectorizer
joblib.dump(model, 'hate_model.pkl')
joblib.dump(vectorizer, 'vectorizer.pkl')

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import joblib
# 
# # Load trained model and vectorizer
# model = joblib.load("hate_model.pkl")
# vectorizer = joblib.load("vectorizer.pkl")
# 
# # App title
# st.title("üö® Hate Speech Detector")
# 
# # Text input field
# text = st.text_area("Enter your text:")
# 
# # Predict button
# if st.button("Detect"):
#     # Vectorize input text
#     vec = vectorizer.transform([text])
#     # Get prediction probability for class 0 (hate speech)
#     proba = model.predict_proba(vec)[0][0]
# 
#     # Custom threshold: 0.4 for stricter detection
#     if proba > 0.4:
#         st.error(f"‚ö†Ô∏è Hate Speech Detected! (Confidence: {proba:.2f})")
#     else:
#         st.success(f"‚úÖ Clean Content (Confidence: {proba:.2f})")
#

from google.colab import files
files.download("app.py")

files.download("hate_model.pkl")
files.download("vectorizer.pkl")

