{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d5de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import normalize\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bcd1c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = 'C:/Users/Asad Computrs/Final Year Project/Brain Tumor classification dataset/'\n",
    "\n",
    "\n",
    "no_tumor = os.listdir(img_dir + 'no/')\n",
    "\n",
    "yes_tumor = os.listdir(img_dir + 'yes/')\n",
    "dataset = []\n",
    "label = []\n",
    "INPUT_SIZE = 64\n",
    "#print(no_tumor)\n",
    "\n",
    "\n",
    "#path = 'no0.jpg'\n",
    "\n",
    "#print(path.split('.')[1]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0952309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , img_name in enumerate(yes_tumor):\n",
    "    if(img_name.split('.')[1] == 'jpg'):\n",
    "        image = cv2.imread(img_dir + 'yes/'+ img_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((INPUT_SIZE, INPUT_SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(0)\n",
    "\n",
    "\n",
    "for i , img_name in enumerate(no_tumor):\n",
    "    if(img_name.split('.')[1] == 'jpg'):\n",
    "        image = cv2.imread(img_dir + 'no/'+ img_name)\n",
    "        image = Image.fromarray(image, 'RGB')\n",
    "        image = image.resize((INPUT_SIZE, INPUT_SIZE))\n",
    "        dataset.append(np.array(image))\n",
    "        label.append(1)\n",
    "\n",
    "\n",
    "#print(len(dataset))\n",
    "#print(len(label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f9b81bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array(dataset)\n",
    "label = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "982cb9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in x_train: 2400\n",
      "Number of samples in y_train: 2400\n",
      "Number of samples in x_test: 600\n",
      "Number of samples in y_test: 600\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset, label, test_size=0.2, random_state=0)\n",
    "\n",
    "# Normalize the data\n",
    "x_train = normalize(x_train)\n",
    "x_test = normalize(x_test)\n",
    "\n",
    "# Check the number of samples in x_train and y_train\n",
    "print('Number of samples in x_train:', len(x_train))\n",
    "print('Number of samples in y_train:', len(y_train))\n",
    "print('Number of samples in x_test:', len(x_test))\n",
    "print('Number of samples in y_test:', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8ea77e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a Model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3,3), input_shape=(INPUT_SIZE, INPUT_SIZE, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, (3,3), kernel_initializer='he_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (3,3), kernel_initializer='he_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fe9608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Binary CrossEntropy = 1 , sigmoid\n",
    "#Cross Entryopy= 2, softmax\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6876b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asad Computrs\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(16, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.4821"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asad Computrs\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 8s 46ms/step - loss: 0.0000e+00 - accuracy: 0.4821 - val_loss: 0.0000e+00 - val_accuracy: 0.5717\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 7s 45ms/step - loss: 0.0000e+00 - accuracy: 0.4821 - val_loss: 0.0000e+00 - val_accuracy: 0.5717\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 7s 44ms/step - loss: 0.0000e+00 - accuracy: 0.4821 - val_loss: 0.0000e+00 - val_accuracy: 0.5717\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 7s 46ms/step - loss: 0.0000e+00 - accuracy: 0.4821 - val_loss: 0.0000e+00 - val_accuracy: 0.5717\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 7s 45ms/step - loss: 0.0000e+00 - accuracy: 0.4821 - val_loss: 0.0000e+00 - val_accuracy: 0.5717\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 7s 47ms/step - loss: 0.0000e+00 - accuracy: 0.4821 - val_loss: 0.0000e+00 - val_accuracy: 0.5717\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 7s 46ms/step - loss: 0.0000e+00 - accuracy: 0.4821 - val_loss: 0.0000e+00 - val_accuracy: 0.5717\n",
      "Epoch 8/10\n",
      " 75/150 [==============>...............] - ETA: 3s - loss: 0.0000e+00 - accuracy: 0.4975"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=16, verbose=1, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4768907a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asad Computrs\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Weights for model 'sequential' have not yet been created. Weights are created when the model is first called on inputs or `build()` is called with an `input_shape`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrain_tumor_cnn_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3730\u001b[0m, in \u001b[0;36mModel._assert_weights_created\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   3721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3722\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[0;32m   3723\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m!=\u001b[39m Model\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3728\u001b[0m     \u001b[38;5;66;03m# Also make sure to exclude Model class itself which has build()\u001b[39;00m\n\u001b[0;32m   3729\u001b[0m     \u001b[38;5;66;03m# defined.\u001b[39;00m\n\u001b[1;32m-> 3730\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3731\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights for model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m have not yet been \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3732\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3733\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights are created when the model is first called on \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3734\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs or `build()` is called with an `input_shape`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3735\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Weights for model 'sequential' have not yet been created. Weights are created when the model is first called on inputs or `build()` is called with an `input_shape`."
     ]
    }
   ],
   "source": [
    "model.save('brain_tumor_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a49ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('brain_tumor_cnn_model.h5')\n",
    "\n",
    "# Function to preprocess the input image\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(64, 64))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "# Paths to your input images\n",
    "base_path = 'C:/Users/Asad Computrs/Final Year Project/Brain Tumor classification dataset/no/'\n",
    "image_paths = [os.path.join(base_path, 'no31.jpg'), os.path.join(base_path, 'no57.jpg')]  # Replace with actual image paths\n",
    "\n",
    "correct_predictions = 0\n",
    "total_images = 0\n",
    "\n",
    "# Iterate through the images and make predictions\n",
    "for img_path in image_paths:\n",
    "    if not os.path.exists(img_path):\n",
    "        print(f\"File not found: {img_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    img = preprocess_image(img_path)\n",
    "\n",
    "    # Predict the class probabilities\n",
    "    predictions = model.predict(img)\n",
    "    \n",
    "    # Assuming binary classification (0: no tumor, 1: tumor)\n",
    "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "\n",
    "    # Display the image\n",
    "    img_display = image.load_img(img_path)\n",
    "    plt.imshow(img_display)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Print the prediction\n",
    "    if predicted_class == 1:\n",
    "        print(\"Prediction: Tumor\")\n",
    "    else:\n",
    "        print(\"No Tumor\")\n",
    "    \n",
    "    # Update accuracy metrics\n",
    "    total_images += 1\n",
    "    if ('no' in img_path and predicted_class == 0) or ('yes' in img_path and predicted_class == 1):\n",
    "        correct_predictions += 1\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (correct_predictions / total_images) * 100\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
